{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IndoDataWeek-NLP-Alireza-solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtyfEbPgjnxSON8b9wCDRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arezaz/nlp-workshop/blob/master/IndoDataWeek_NLP_Alireza_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TAi_alZvBCt"
      },
      "source": [
        "#**The Natural Language Processing Workshop**\n",
        "---\n",
        "#### Alireza Rezazadeh (rezaz003@umn.edu)\n",
        "Indo Data Week - November, 2020\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfOCTOo2zuw9"
      },
      "source": [
        "## **1. Word Representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSh5jrN4u9uW"
      },
      "source": [
        "#### **Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwWR33Nctr01"
      },
      "source": [
        "\n",
        "1. Word Representation:\n",
        "  * 1.1. Basic Approaches\n",
        "      * 1.1.1. One-Hot Encoding\n",
        "      * 1.1.2. Bag of Words\n",
        "      * 1.1.3. TF-IDF\n",
        "  * 1.2. Word Embedding\n",
        "      * 1.2.1. Word2Vec\n",
        "      * 1.2.2. GloVe\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7HhWYa2-9UM"
      },
      "source": [
        "#### **1.1. Basic Aproaches**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVx_bhd2gas"
      },
      "source": [
        "*Vector space model* is an algebraic model of representing text as vecotrs of identifiers. All following approaches discussed here are one way or another based on the idea of vector space models. Basic idea:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/1.png\" alt=\"Drawing\" width=\"350\" />\n",
        "\n",
        "The most basic idea: assigning unique ID to each word in the vocabulary. This way, we can represent text as a multi-dimensional vector. Let's go over a few ways to implement this idea!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRiwPax0BSHC"
      },
      "source": [
        "##### **1.1.1. One-Hot Encoding**\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/2.png\" alt=\"Drawing\" width=\"700\" />\n",
        "\n",
        "Let's try this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQXHKNJkG0fC"
      },
      "source": [
        "VOCAB = {'yellow':0, 'apple':1, 'dog':2, 'cat':3, 'blue':4, 'black':5, 'sky':6}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGOwFx55u49d"
      },
      "source": [
        "def onehot(text):\n",
        "  onehot_rep = []\n",
        "  for word in text.split():\n",
        "    # initializing one-hot vector\n",
        "    vec = [0]*len(VOCAB)\n",
        "    if word in VOCAB.keys():\n",
        "      # building one-hot vector\n",
        "      vec[VOCAB[word]] = 1\n",
        "    onehot_rep.append(vec)\n",
        "  return onehot_rep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbIq3DFvIG_C",
        "outputId": "55333504-9627-4b4b-d179-b2273de0933b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "onehot('the sky is blue and the cat is black')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 1, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_J4UsEeIYrn"
      },
      "source": [
        "Why is this approach not so practical?\n",
        "  * High dimensionality of the representation.\n",
        "  * Sparsity leads to computational difficaulties.\n",
        "  * Does not capture semantic relation between words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvXYti6hJjTo"
      },
      "source": [
        "##### **1.1.2. Bag of Words**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/3.png\" alt=\"Drawing\" width=\"700\" />\n",
        "\n",
        "Let's try this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrGhyOkuIKre",
        "outputId": "bc3db3b8-f6ea-483b-cc6d-af3fad53d5a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [ 'This is the first document and it is interesting.',\n",
        "           'This document is the second document and it is in English.',\n",
        "           'And this is the third one.',\n",
        "           'Is this the first document?']\n",
        "\n",
        "\n",
        "count_vect = CountVectorizer() # CountVectorizer(binary=True)\n",
        "\n",
        "# Building bag-of-word representation of the corpus\n",
        "bow_rep = count_vect.fit_transform(corpus)\n",
        "\n",
        "# Extracting the vocabulary\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "# Bag-of-words representation of documents\n",
        "print(\"BoW of '\" + corpus[0] + \"': \", bow_rep[0].toarray())\n",
        "print(\"BoW of '\" + corpus[1] + \"': \",bow_rep[1].toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our vocabulary:  {'this': 12, 'is': 6, 'the': 10, 'first': 3, 'document': 1, 'and': 0, 'it': 7, 'interesting': 5, 'second': 9, 'in': 4, 'english': 2, 'third': 11, 'one': 8}\n",
            "BoW of 'This is the first document and it is interesting.':  [[1 1 0 1 0 1 2 1 0 0 1 0 1]]\n",
            "BoW of 'This document is the second document and it is in English.':  [[1 2 1 0 1 0 2 1 0 1 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC4pPgN_UsR3",
        "outputId": "e35280e0-caa6-4591-a933-c1aa0a45895c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Bag-of-words representation of a new documents\n",
        "NewDocument = \"dog and dog are friends\"\n",
        "temp = count_vect.transform([NewDocument])\n",
        "print(\"BoW of '\" + NewDocument + \"': \",temp.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BoW of 'dog and dog are friends':  [[1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVtSkBv6GBNv"
      },
      "source": [
        "A few advantages:\n",
        "  *   Straight-forward and easy to implement.\n",
        "  *   Captures semantic similarity for documents.\n",
        "  *   Encoding is fixed length for any document.\n",
        "\n",
        "Some drawbacks:\n",
        "  *   Vectors could become sparse for larger vocabulary.\n",
        "  *   Blind! cannot capture semantic similarity of words.\n",
        "  *   What if a new word is not in the vocabulary?\n",
        "  *   It's a bag! disregards words order information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9IDS6sCJ9Su"
      },
      "source": [
        "##### **1.1.3. TF-IDF**\n",
        " \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/4.png\" alt=\"Drawing\" width=\"1400\" />\n",
        "\n",
        "Let's try this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6oSeZKlB5tw",
        "outputId": "2baf60a3-2d66-4317-cbb2-8b88e25ec2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "bow_rep_tfidf = tfidf.fit_transform(corpus)\n",
        "\n",
        "df_temp = pd.DataFrame({'Vocab': tfidf.get_feature_names(), 'IDF':tfidf.idf_}) # features and their IDF\n",
        "df_temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Vocab</th>\n",
              "      <th>IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>and</td>\n",
              "      <td>1.223144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>document</td>\n",
              "      <td>1.223144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>english</td>\n",
              "      <td>1.916291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>first</td>\n",
              "      <td>1.510826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in</td>\n",
              "      <td>1.916291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>interesting</td>\n",
              "      <td>1.916291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>is</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>it</td>\n",
              "      <td>1.510826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>one</td>\n",
              "      <td>1.916291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>second</td>\n",
              "      <td>1.916291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>the</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>third</td>\n",
              "      <td>1.916291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>this</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Vocab       IDF\n",
              "0           and  1.223144\n",
              "1      document  1.223144\n",
              "2       english  1.916291\n",
              "3         first  1.510826\n",
              "4            in  1.916291\n",
              "5   interesting  1.916291\n",
              "6            is  1.000000\n",
              "7            it  1.510826\n",
              "8           one  1.916291\n",
              "9        second  1.916291\n",
              "10          the  1.000000\n",
              "11        third  1.916291\n",
              "12         this  1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikqpvJ_sURfu",
        "outputId": "cfdf747b-e178-4cb5-fd28-50f57732b73b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "NewDocument = \"This English document is interesting\"\n",
        "temp = tfidf.transform([NewDocument])\n",
        "print(\"TF-IDF for '\"+NewDocument+\"' :\\n\", temp.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF for 'This English document is interesting' :\n",
            " [[0.         0.37149619 0.58202057 0.         0.         0.58202057\n",
            "  0.30372248 0.         0.         0.         0.         0.\n",
            "  0.30372248]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UXKWAKgW_-k"
      },
      "source": [
        "There are a few variations of TF-IDF formulations. Advantages of this approach are:\n",
        "*   Like bag-of-words, it captures semantic similarity for documents.\n",
        "*   Very commonly used (even today!).\n",
        "\n",
        "But, like any other approach, there are some drawbacks:\n",
        "*   Curse of dimensionality! feature vectors can become very high dimensional.\n",
        "*   Still doesn't capture semantic relation between words.\n",
        "*   Sparsity in feature vecotors.\n",
        "*   Does not handle out-of-vocabulary words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VZBixsGKc_E"
      },
      "source": [
        "#### **1.2. Word Embeddings**\n",
        "\n",
        "Words that can be used interchangably often have a strong semantic relation. \n",
        "\n",
        "> The [*cat*, *dog*] is a [*domestic*, *wild*] species of [*small*, *large*] carnivorous mammal.  - *Wikipedia*\n",
        "\n",
        "Word embeddings represent words as real-valued vectors in a predifined vector space. In particular, word embeddings are from the class of dense distributed representation. Unlike sparse representations, such as one-hot encoding, distributed representations are learned based on the usage of words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrFE_yPkWnJy"
      },
      "source": [
        "##### **1.2.1. Word2Vec**\n",
        " \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/5.png\" alt=\"Drawing\" width=\"1700\" />\n",
        "\n",
        "Word embedding models are usually trained on huge datasets with large vocabulary size. Luckily we don't need to train neural networks from scratch Let's try a pre-trained model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJGp2genc6a7"
      },
      "source": [
        "from gensim.models import Word2Vec \n",
        "from gensim.test.utils import common_texts\n",
        "\n",
        "# Build a Word2Vec model on a costume dataset\n",
        "model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLeFbFbRLDnr",
        "outputId": "25628165-6be7-45ec-890f-9307514de1a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Finding similar words\n",
        "w = 'human'\n",
        "print(w + ' is similar to: ' + str(model.wv.most_similar(w, topn=3)))\n",
        "\n",
        "# the representation for one of the words\n",
        "print(w + ' vector is: ' + str(model[w]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "human is similar to: [('computer', 0.7257747650146484), ('system', 0.3190498352050781), ('interface', 0.20291012525558472)]\n",
            "human vector is: [ 0.0156147   0.04177426  0.02856586 -0.03033977  0.01721545 -0.01652424\n",
            "  0.0282451   0.02292306  0.01094562  0.04534782]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6HD6l3DMLi4"
      },
      "source": [
        "##### **1.2.2. GLoVe**\n",
        " \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/6.png\" alt=\"Drawing\" width=\"1700\" />\n",
        "\n",
        "Let's try a pre-trained GLoVe model based on twitter data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLPvi6ULvhr",
        "outputId": "e5e13495-c0b3-46dc-e2cc-249b468be9b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# loading GloVe model trained on tweets\n",
        "glove_model = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "# Finding similar words\n",
        "w = 'computer'\n",
        "print(w + ' is similar to: ' + str(glove_model.most_similar(w, topn=5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "computer is similar to: [('camera', 0.907833456993103), ('cell', 0.891890287399292), ('server', 0.8744666576385498), ('device', 0.869352400302887), ('wifi', 0.8631256818771362)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FQKjGl-Up12"
      },
      "source": [
        "To measure similarity between two vectors we can use *cosine similarity* metric. It is defined to equal the cosine of the angle between them, which is also the same as the inner product of the same vectors normalized to both have length 1.\n",
        "\n",
        "<img src=\"https://www.oreilly.com/library/view/mastering-machine-learning/9781785283451/assets/d258ae34-f4f8-4143-b3c2-0cb10f2b82de.png\" alt=\"Drawing\" width=\"800\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr5cp7YWNLRs",
        "outputId": "fba441d8-3405-4823-bf29-be2ee67f31d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "glove_model[w]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.64005 , -0.019514,  0.70148 , -0.66123 ,  1.1723  , -0.58859 ,\n",
              "        0.25917 , -0.81541 ,  1.1708  ,  1.1413  , -0.15405 , -0.11369 ,\n",
              "       -3.8414  , -0.87233 ,  0.47489 ,  1.1541  ,  0.97678 ,  1.1107  ,\n",
              "       -0.14572 , -0.52013 , -0.52234 , -0.92349 ,  0.34651 ,  0.061939,\n",
              "       -0.57375 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlcTq_l-V9Mt"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "# similarity function based on cosine similarity\n",
        "def similarity(w1,w2):\n",
        "  return 1 - spatial.distance.cosine(glove_model[w1],glove_model[w2])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5c3rLTqWXDV",
        "outputId": "e848787d-b8bf-480b-8413-a85d77d73ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w1s, w2s = 'man', 'woman'\n",
        "w1d, w2d = 'man', 'umbrella'\n",
        "\n",
        "print(f\"similarity({w1s}, {w2s}) = \", similarity(w1s, w2s))\n",
        "print(f\"similarity({w1d}, {w2d}) = \", similarity(w1d, w2d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "similarity(man, woman) =  0.7654176354408264\n",
            "similarity(man, umbrella) =  0.4887590706348419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z569ldFWsdm",
        "outputId": "7c3fe85e-1bc3-4f31-c3b9-5f4b2ea6ebd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w1s, w2s = 'small', 'smaller'\n",
        "w1d, w2d = 'large', 'larger'\n",
        "\n",
        "print(f\"similarity({w1s}, {w2s}) = \", similarity(w1s, w2s))\n",
        "print(f\"similarity({w1d}, {w2d}) = \", similarity(w1d, w2d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "similarity(small, smaller) =  0.8645442128181458\n",
            "similarity(large, larger) =  0.8587130904197693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APm3BHWvZCxR"
      },
      "source": [
        "This is interesting. But wait! does data equally favor everyone? What if we want to somehow use a model trained on a large dataset of tweets to make a decision for the society? \n",
        "\n",
        "Let's take a look at how language can be unfair and biased! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Y3-toPY0yy",
        "outputId": "631b55db-2fdb-4f32-bb22-fc7068211855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "career = 'doctor'\n",
        "\n",
        "male_names = ['john', 'carl', 'steven', 'ashton', 'alireza', 'mohammed', 'ishaan']\n",
        "female_names = ['rose', 'emma', 'lily', 'kylie', 'yasmin', 'zahra', 'shanaya']\n",
        "\n",
        "print('~~~~~~~~~ Male names ~~~~~~~~~')\n",
        "for name in male_names:\n",
        "  print(f\"s({name}, {career}) ---> {round(similarity(name, career),3)}\")\n",
        "\n",
        "print('\\n~~~~~~~~~ Female names ~~~~~~~~~')\n",
        "for name in female_names:\n",
        "  print(f\"s({name}, {career}) ---> {round(similarity(name, career),3)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~ Male names ~~~~~~~~~\n",
            "s(john, artist) ---> 0.686\n",
            "s(carl, artist) ---> 0.549\n",
            "s(steven, artist) ---> 0.595\n",
            "s(ashton, artist) ---> 0.573\n",
            "s(alireza, artist) ---> 0.039\n",
            "s(mohammed, artist) ---> 0.276\n",
            "s(ishaan, artist) ---> -0.128\n",
            "\n",
            "~~~~~~~~~ Female names ~~~~~~~~~\n",
            "s(rose, artist) ---> 0.575\n",
            "s(emma, artist) ---> 0.649\n",
            "s(lily, artist) ---> 0.588\n",
            "s(kylie, artist) ---> 0.667\n",
            "s(yasmin, artist) ---> 0.331\n",
            "s(zahra, artist) ---> 0.283\n",
            "s(shanaya, artist) ---> -0.113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYJGwtcTdnZH"
      },
      "source": [
        "We experimented with one of the common word embedding models and observed how based on biased data a machine learning model can be biased. Recently, the subject of AI fairness has been attracting more attention.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "105L3Kg7eujE"
      },
      "source": [
        "## **2. Language Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aupwnnrvapFI"
      },
      "source": [
        "#### **Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvZtzYhbapFJ"
      },
      "source": [
        "2. Language Models\n",
        "  * 2.1 Basic Models\n",
        "      * 2.1.1. Unigram/Bigram Models\n",
        "      * 2.1.2. N-grams Models\n",
        "  * 2.2 Recurrent Nerual Networks (RNN)\n",
        "      * 2.2.1. Basic RNN\n",
        "      * 2.2.2. Vanilla RNN\n",
        "      * 2.2.3. LSTM\n",
        "      * 2.2.4. Deep RNN\n",
        "\n",
        "* Hands-on: Building a Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmVnXg5U7rZv"
      },
      "source": [
        "#### **2.1. Basic Models**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NlrkdWm17WX"
      },
      "source": [
        "Language models, originally developed for speech recognition, are widely used in many NLP applications. Probabilistic language models compute the probability of a sentence or sequence of words. Specifically, language models can be developed to predict the likelihood of a given word, or a sequence of words, to follow a sequence of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXjBHAR3Fxie"
      },
      "source": [
        "##### **2.1.1. Unigram/Bigram Models**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/7.png\" alt=\"Drawing\" width=\"1700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9mYKBHwF8sd"
      },
      "source": [
        "##### **2.1.2. N-gram Models**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/arezaz/nlp-workshop/master/contents/8.png\" alt=\"Drawing\" width=\"1800\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KoZRqAzbiLO",
        "outputId": "e6a60fe0-aa5e-49bf-b1de-2b233688f75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# function to extract n-grams of sentences\n",
        "def ngrams_gen(seq, n):\n",
        "    n_grams = ngrams(nltk.word_tokenize(seq), n)\n",
        "    return [ ' '.join(grams) for grams in n_grams]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKf2jsYZKf03",
        "outputId": "0a2d4a37-d220-41f0-b350-176f4e896d17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence = 'The whole is more than the sum of its parts' # -Aristotle\n",
        "\n",
        "n = [1, 2, 3, 4]\n",
        "for _ in n:\n",
        "  print(str(_)+\"-gram: \", ngrams_gen(sentence, _))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram:  ['The', 'whole', 'is', 'more', 'than', 'the', 'sum', 'of', 'its', 'parts']\n",
            "2-gram:  ['The whole', 'whole is', 'is more', 'more than', 'than the', 'the sum', 'sum of', 'of its', 'its parts']\n",
            "3-gram:  ['The whole is', 'whole is more', 'is more than', 'more than the', 'than the sum', 'the sum of', 'sum of its', 'of its parts']\n",
            "4-gram:  ['The whole is more', 'whole is more than', 'is more than the', 'more than the sum', 'than the sum of', 'the sum of its', 'sum of its parts']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQCMAJlBLvLj"
      },
      "source": [
        "In general, N-gram models are not sufficient models for language altough they are very useful. Language is essentially dependent on complicated long-distance histories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3pSNeKLJBN"
      },
      "source": [
        "#### **2.2. Recurrent Neural Networks**\n",
        "\n",
        "As mentioned earlier, language is sequential at its essense. *Recurrent neural networks* (RNN) are specifically developed to capture a sequential process. In general, RNN is composed of units that keep a memory of preceeding history. This memory is temporal as the units keep updating at every timestep. RNN are very promising in many NLP applications.\n",
        "\n",
        "\n",
        "*images courtesy of cs230 - stanford, and colah.github.io*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7MAUACyMz2l"
      },
      "source": [
        "##### **2.2.1. Basic RNN**\n",
        "\n",
        "At each timestep $t$, the unit takes input $x^{<t>}$, outputs $y^{<t>}$, and pass a signal $a^{<t>}$ based on its computations to the next timestep $t+1$. This signal acts as a memory in the sequence.\n",
        "\n",
        "\n",
        "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png?9ea4417fc145b9346a3e288801dbdfdc\" alt=\"Drawing\" width=\"600\" />\n",
        "\n",
        "Specifically, \n",
        "\n",
        "$a^{<t>}=g_a(W_a [a^{<t-1>}, x^{<t>}]+b_a)$\n",
        "\n",
        "$\\hat{y}^{<t>}=g_y(W_y a^{<t>}+b_y)$\n",
        "\n",
        "The loss function is defined as: $L(\\hat{y}-y) = \\sum_{t=1}^{T}L(\\hat{y}^{<t>}-y^{<t>}) $. The RNN network is trained by *backpropagation through time*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAMamkSbY6Ja"
      },
      "source": [
        "Type | Schematic | Application\n",
        " --- |    ---    |     ---\n",
        "one-to-one | <img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-one-ltr.png?9c8e3b04d222d178d6bee4506cc3f779\" alt=\"Drawing\" width=\"400\" /> | a basic nerual network \n",
        "one-to-many | <img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-one-to-many-ltr.png?d246c2f0d1e0f43a21a8bd95f579cb3b\" alt=\"Drawing\" width=\"400\" /> | music generation\n",
        "many-to-one | <img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-one-ltr.png?c8a442b3ea9f4cb81f929c089b910c9d\" alt=\"Drawing\" width=\"400\" /> | sentiment classification\n",
        "many-to-many | <img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-same-ltr.png?2790431b32050b34b80011afead1f232\" alt=\"Drawing\" width=\"400\" /> | named-entity recognition (NER)\n",
        "many-to-many | <img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-different-ltr.png?8ca8bafd1eeac4e8c961d9293858407b\" alt=\"Drawing\" width=\"400\" /> | machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdaLEvYNqrLm"
      },
      "source": [
        "##### **2.2.2. Vanilla RNN**\n",
        "\n",
        "Vanilla RNN is a basic RNN unit. The non-linearity function is set to $tanh$ and can be described as:\n",
        "\n",
        "$a^{<t>}=tanh(W_a[a^{<t-1>}, x^{<t>}]+b_a)$\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" alt=\"Drawing\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbmjA454tGkS"
      },
      "source": [
        "##### **2.2.3. LSTM**\n",
        "\n",
        "LSTM (long short-term memory) network introduced back in 1990s are extremely promising in a wide variety of sequential modelings. They are formulated to allow for learning lags of unknown duration between relevant events in a sequential data. A major feature of LSTMs is their capability in experssing the notion of forgetting.\n",
        "\n",
        "There are many variations of LSTMs. In general, an LSTM cell has three main components, also called *gates*: input gate, output gate, and forget gate. On top of that, each cell has a *state* associated with it.\n",
        "\n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt=\"Drawing\" width=\"600\" />\n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png\" alt=\"Drawing\" width=\"500\" />\n",
        "\n",
        "\n",
        "So, what is the main idea?\n",
        "\n",
        "The cell state runs through all timesteps and the cell can add or remove information to the state signal. The three gates allows the cell to arbitrarily let information through. Gates are composed of sigmoid function to determine how much of each component let through. \n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" alt=\"Drawing\" width=\"700\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8AoBAG1y2b4"
      },
      "source": [
        "Let's briefly walk through an LSTM cell:\n",
        "\n",
        "*1) Decide what to forget:* look at the previous timestep hidden state $h_{t-1}$ and the input at this timestep $x_t$ and through a sigmoid layer determine how much to forget *(forget gate)*: \n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" alt=\"Drawing\" width=\"700\" />\n",
        "\n",
        "*2) Decide what to remember:* look at the previous timestep hidden state $h_{t-1}$ and the input at this timestep $x_t$. First, decide which values to update through a sigmoid layer *(input gate)*. Then, compute the candidate values for the updates: \n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" alt=\"Drawing\" width=\"700\" />\n",
        "\n",
        "*3) Update cell states:* now that we know what to remember and what to forget we can impose them to the cell state signal: \n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" alt=\"Drawing\" width=\"700\" />\n",
        "\n",
        "*4) Come up with an output:* look at the previous timestep hidden state $h_{t-1}$ and through a sigmoid layer decide what parts of this signal to update and output (*output gate*). Then, combine this signal with the candidate update values and compute the hidden state at this timestep $h_{1}$: \n",
        "\n",
        "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" alt=\"Drawing\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI0y1qHT9JbR"
      },
      "source": [
        "As mentioned before, LSTMs are great and promising in general but we should also be aware of some of their drawbacks:\n",
        "\n",
        "* LSTMs are developed to solve the problem of vanishing gradients in the process of backpropagation. However, they also suffer from vanishing gradient problem to some extent.\n",
        "* The cell function is quite complex and this can exacerbate the issue of vanishing gradients.\n",
        "* Computationally expensive to train up to a decent performance level.\n",
        "* Cannot remember history of sequence for relatively longer timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrMPWDe6-c6n"
      },
      "source": [
        "##### **2.2.4. Deep RNN**\n",
        "\n",
        "How do we make an RNN deep? just stack them up and build layers on top of each other!\n",
        "\n",
        "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/deep-rnn-ltr.png?f57da6de44ddd4709ad3b696cac6a912\" alt=\"Drawing\" width=\"400\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLpsws9nAL5E"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isu0GoCnAPBs"
      },
      "source": [
        "## 3. Hands-on: Working with Transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgTBHHqelZi-"
      },
      "source": [
        "### So, what are the Transformers really?\n",
        "\n",
        "Transformer in natural language processing refers to state of the art sequence-to-sequence language models. The backbone of transformers is *attention mechanism* to handle long distance dependencies. \n",
        "\n",
        "Attention mechanism basically weights different positions of a single sequence in order to compute a representation of the sequence. Here's how it looks:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*wa4zt-LcMWRIYLfiHfBKvA.png\" alt=\"Drawing\" width=\"450\" />\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK4Pnym4vrGG"
      },
      "source": [
        "### 3.1. GPT-2\n",
        "Generative  Pre-trained Transformers (GPT) are developed by OpenAI. GPT-2 is released in 4 different variations based on their size (small, medium, large, and XL). GPT-2 is a very large model with 1.5 billion parameters. The pre-trained models contain information from 8 million web pages from. GPT-2 is an unsupervised language model announced in 2019. Building blocks of GPT-2 is:\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/795bd8868fdeb49b7ca48a935e806b56169a172b/68747470733a2f2f692e696d6775722e636f6d2f305853535842642e706e67\" alt=\"Drawing\" width=\"180\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9-3C7TEy8vm"
      },
      "source": [
        "#### 3.1.1. Next Word Generation Using GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3ZkEp2NCFBj",
        "outputId": "8a2811e3-6a03-4791-c894-489638fe8ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 9.1MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/59/ac498beaa03fe70f123e91bc80552e810422e3305bfc67987799005b80ee/boto3-1.16.16-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/af/deadbd11bb212b03554bdb249fe574e402452459d2d7c3aabba1811bcf84/botocore-1.19.16-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 16.9MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.16->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b4399bee5825c78a01ba9643a8c3e825de7c8dcd995add8eb926e7f5bf348cc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.16 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, jmespath, botocore, s3transfer, boto3, sacremoses, pytorch-transformers\n",
            "Successfully installed boto3-1.16.16 botocore-1.19.16 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uid3k0Z0FBb"
      },
      "source": [
        "# importing GPT2 library\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oFKm95w0FDV",
        "outputId": "530a2617-b1bd-4d21-db56-519990debe30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating an instance of GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:00<00:00, 12553164.05B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 6607595.92B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lhHdQlU0g-4",
        "outputId": "190e3672-7051-46b3-b1cb-14bf621c8690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loading a pre-trained GPT-2 model with multi-head attention mechanism\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 665/665 [00:00<00:00, 129513.94B/s]\n",
            "100%|██████████| 548118077/548118077 [00:13<00:00, 42113603.07B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plo02bnR0FHq",
        "outputId": "5c0de0c3-b46f-494c-b71c-9d3e0bac2029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# let's predict the next word in this sequence\n",
        "text = \"artificial inteligence aims to predict the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "print(f'tokens are: {indexed_tokens}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens are: [11666, 33649, 328, 594, 12031, 284, 4331, 262]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FoojtWG1Nyh"
      },
      "source": [
        "# Predicting the next word in the sequence\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7VRW-q01PDg",
        "outputId": "e04df90b-638b-45c8-d14a-b77345273628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "predicted_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' artificial inteligence aims to predict the future'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACKqs1Gb18k7"
      },
      "source": [
        "#### 3.1.2. Next Sentences Generation Using GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw6MNchq1PFw"
      },
      "source": [
        "start = 'Intelligence is most often studied in humans but has also been observed in both'\n",
        "indexed_tokens = tokenizer.encode(start)\n",
        "\n",
        "for i in range(150):\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "    predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "    indexed_tokens = indexed_tokens + [predicted_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_lB3EDg1PIR",
        "outputId": "519a9a5f-c277-490d-8d30-cae62ec0fb7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "print(predicted_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Intelligence is most often studied in humans but has also been observed in both animals and humans.\n",
            "\n",
            "The study, published in the journal Nature Communications, found that the brain of a human with autism was more sensitive to the presence of a chemical called methylphenidate, which is found in the brain of a human with autism.\n",
            "\n",
            "\"This is the first time that we have found that the brain of a human with autism is more sensitive to methylphenidate than that of a human with autism,\" said lead author Dr. David H. Hirsch, a professor of psychiatry at the University of California, San Francisco.\n",
            "\n",
            "The study was conducted in a laboratory at the University of California, San Francisco.\n",
            "\n",
            "The researchers found that the brain of a human with autism was more sensitive to methylphenidate than that that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQZlGf5p3sVt"
      },
      "source": [
        "---\n",
        "## Relevant Materials\n",
        "* Vajjala, Majumder, Gupta, Surana - [Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems](https://www.amazon.com/Practical-Natural-Language-Processing-Pragmatic/dp/1492054054)\n",
        "* [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) by deeplearning.ai\n",
        "* [Stanford's CS230](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) course materials\n",
        "* Christopher Olah's [blog](https://colah.github.io/)\n"
      ]
    }
  ]
}